# ğŸµ Gesture-Controlled Spotify AI

By Siddharth Inamdar

---

## ğŸ§© Overview
A real-time AI system that controls Spotify using hand gestures. Built with OpenCV, MediaPipe, and a KNN classifier for fast, reliable predictions.

- Dataset: 2115 manually captured samples
- Accuracy: 94.9% (KNN)
- Features: 63 coordinates per frame (21 landmarks Ã— x,y,z)

---

## ğŸ¥ Demo
ğŸ¬ Watch Demo Video:I have added the video in "Uploaded" folder. Please download the video to watch.
This demo showcases full control of Spotify through gestures such as play/pause, like, volume control, and track switching.

---

## ğŸ–ï¸ Supported Gestures
| Gesture | Action |
|---|---|
| ğŸ‘ Thumbs Up | Play |
| ğŸ–ï¸ Open Palm | Pause |
| ğŸ¤˜ Rock On | Like Song |
| â˜ï¸ Index Swipe Up | Volume Up |
| ğŸ‘‡ Index Swipe Down | Volume Down |
| ğŸ‘‰ Swipe Right | Next Song |
| ğŸ‘ˆ Swipe Left | Previous Song |

---

## âš™ï¸ Tech Stack
- Python 3.11+
- OpenCV â€” Captures real-time frames
- MediaPipe â€” Detects 21 3D hand landmarks
- Scikit-learn (KNN) â€” Gesture classification
- PyAutoGUI â€” Automates Spotify keyboard shortcuts
- PyGetWindow (optional) â€” Window focus management
- Pickle â€” Saves trained ML model

---

## ğŸ§  Model Summary
- Dataset: 2115 manually labeled samples
- Model: KNN Classifier
- Accuracy: 94.9%
- Features: 63 numerical coordinates (21 landmarks Ã— 3 axes)

---

## ğŸš€ How It Works
1) Run `collect_data.py` â†’ Collect gesture data
2) Run `train_model.py` â†’ Train KNN model
3) Run `main.py` â†’ Control Spotify in real-time

Each frame is converted into 63 numerical coordinates by MediaPipe, classified by the ML model, and translated into Spotify actions.

---

## ğŸ”§ Project Architecture
```
Camera â†’ MediaPipe (21 Landmarks) â†’ 63 Features â†’ KNN Model â†’ Action â†’ Spotify
```

---

## ğŸ“¦ Project Structure
```
Gesture Controlled Spotify/
â”œâ”€â”€ collect_data.py           # Dataset collection
â”œâ”€â”€ train_model.py            # Model training
â”œâ”€â”€ main.py                   # Real-time gesture control
â”œâ”€â”€ gesture_model.pkl         # Trained ML model
â”œâ”€â”€ gestures.csv              # Dataset (generated)
â”œâ”€â”€ README.md                 # Portfolio documentation
â””â”€â”€ Problem Solving and Learnings/
    â”œâ”€â”€ problems_and_solutions.txt
    â””â”€â”€ project_learnings.txt
```

---

## â¤ï¸ Key Learnings
- I learned how to extract and use real-time hand landmarks with MediaPipe.
- I understood how classical ML (KNN) can outperform heavier models for lightweight, real-time tasks.
- I discovered the importance of buffer and cooldown logic for gesture stability.
- I learned to optimize FPS for real-time AI systems without GPU dependency.

---

## ğŸ“ˆ Future Enhancements
- Add minimize/maximize gestures ğŸªŸ
- Integrate TensorFlow for CNN-based deep learning recognition ğŸ§ 
- Add dashboard visualization for gesture detection accuracy ğŸ“Š

---

## ğŸ Final Details
- Dataset: 2115 samples
- Accuracy: 94.9%
- Language: Python
- Author: Siddharth Inamdar
- Completion: November 2025

---

## ğŸ“œ License
Open-source project â€” Free to use for educational purposes. Attribution appreciated.
